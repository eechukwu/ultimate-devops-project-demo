name: "Terraform AWS Deployment and Application Deployment"

on:
  workflow_dispatch:
    inputs:
      confirm_deploy:
        description: 'Type "DEPLOY" to confirm deployment'
        required: true
        type: string

env:
  AWS_REGION: eu-west-2
  TERRAFORM_DIR: ./terraform
  K8S_DIR: ./kubernetes

permissions:
  id-token: write
  contents: read

jobs:
  terraform:
    name: "Terraform"
    runs-on: ubuntu-latest
    if: github.event.inputs.confirm_deploy == 'DEPLOY'

    # Add this outputs block
    outputs:
      cluster_name: ${{ steps.tf-output.outputs.cluster_name }}
      vpc_id: ${{ steps.tf-output.outputs.vpc_id }}

    steps:
      # Check out the repository code
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Set up AWS credentials using OIDC
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # Install Terraform on the runner
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      # Initialize Terraform working directory
      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform init
          echo "Checking Terraform state:"
          terraform state list || echo "No state found"

      # Create Terraform plan
      - name: Terraform Plan
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform plan -out=tfplan

      # Apply Terraform changes
      - name: Terraform Apply
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: terraform apply -auto-approve tfplan

      # Retrieve Terraform outputs (VPC ID and cluster name)
      - name: Get Terraform Outputs
        id: tf-output
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          VPC_ID=$(terraform output -raw vpc_id)
          CLUSTER_NAME=$(terraform output -raw cluster_name)
          echo "vpc_id=${VPC_ID}" >> $GITHUB_OUTPUT
          echo "cluster_name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT

      # Verify EKS Cluster status
      - name: Verify EKS Cluster
        run: |
          aws eks describe-cluster --name ${{ steps.tf-output.outputs.cluster_name }} --query 'cluster.status'
          

      # Wait for EKS Cluster to be Active
      - name: Wait for EKS Cluster to be Active
        run: |
          while [[ $(aws eks describe-cluster --name ${{ steps.tf-output.outputs.cluster_name }} --query 'cluster.status' --output text) != "ACTIVE" ]]; do
            echo "Waiting for EKS cluster to be active..."
            sleep 30
          done

  kubernetes:
    name: Kubernetes Deployment
    needs: terraform
    runs-on: ubuntu-latest

    env:
      EKS_CLUSTER_NAME: ${{ needs.terraform.outputs.cluster_name }}

    steps:
      # Check out the repository code
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Set up AWS credentials using OIDC
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # Update kubeconfig to connect to the EKS cluster
      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ needs.terraform.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      # Install kubectl on the runner
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      # Install Helm on the runner
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      # Retrieve the OIDC ID for the EKS cluster
      - name: Get OIDC ID
        id: oidc
        run: |
          OIDC_ID=$(aws eks describe-cluster --name ${{ needs.terraform.outputs.cluster_name }} \
            --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
          echo "oidc_id=${OIDC_ID}" >> $GITHUB_OUTPUT

      - name: Install eksctl
        run: |
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz"
          tar -xzf eksctl_$(uname -s)_amd64.tar.gz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version

      # Configure the OIDC provider for the EKS cluster
      - name: Configure OIDC Provider
        run: |
          eksctl utils associate-iam-oidc-provider \
            --cluster ${{ needs.terraform.outputs.cluster_name }} \
            --region ${{ env.AWS_REGION }} \
            --approve

      # Create an IAM policy for the AWS Load Balancer Controller
      - name: Create IAM Policy for Load Balancer Controller
        run: |
          curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
          aws iam create-policy \
            --policy-name AWSLoadBalancerControllerIAMPolicy \
            --policy-document file://iam_policy.json || true

      # Create a service account for the AWS Load Balancer Controller
      - name: Create Service Account for Load Balancer Controller
        run: |
          # Create IAM service account with override
          eksctl create iamserviceaccount \
            --cluster=${{ needs.terraform.outputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole \
            --attach-policy-arn=arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
            --region ${{ env.AWS_REGION }} \
            --override-existing-serviceaccounts \
            --approve

      # Verify AWS Load Balancer Controller Service Account
      - name: Verify Load Balancer Controller Service Account
        run: |
          # Create Kubernetes service account if it doesn't exist
          kubectl create serviceaccount aws-load-balancer-controller -n kube-system --dry-run=client -o yaml | kubectl apply -f -

          # Verify the service account
          kubectl get serviceaccount aws-load-balancer-controller -n kube-system

      - name: Configure aws-auth ConfigMap
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          # Update kubeconfig
          aws eks update-kubeconfig --name ${{ steps.tf-output.outputs.cluster_name }} --region ${{ env.AWS_REGION }}
          
          # Get role ARNs from Terraform
          ADMIN_ROLE_ARN=$(terraform output -raw eks_admin_role_arn)
          DEVELOPER_ROLE_ARN=$(terraform output -raw eks_developer_role_arn)
          READONLY_ROLE_ARN=$(terraform output -raw eks_readonly_role_arn)
          
          # Create ConfigMap manifest
          cat << EOF > aws-auth-configmap.yaml
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: aws-auth
            namespace: kube-system
          data:
            mapRoles: |
              - rolearn: ${ADMIN_ROLE_ARN}
                username: eks-admin
                groups:
                  - system:masters
              - rolearn: ${DEVELOPER_ROLE_ARN}
                username: eks-developer
                groups:
                  - developer
              - rolearn: ${READONLY_ROLE_ARN}
                username: eks-readonly
                groups:
                  - readonly
          EOF
          
          # Apply the ConfigMap
          kubectl apply -f aws-auth-configmap.yaml

      # Add and update the Helm repository
      - name: Add and Update Helm Repo
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update eks

      # Install the AWS Load Balancer Controller using Helm
      - name: Install AWS Load Balancer Controller
        run: |
          # Install with extended timeout
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ needs.terraform.outputs.cluster_name }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --set vpcId=${{ needs.terraform.outputs.vpc_id }} \
            --wait \
            --timeout 15m
      
          # Verify deployment
          kubectl get deployment aws-load-balancer-controller -n kube-system
          kubectl describe deployment aws-load-balancer-controller -n kube-system

      # Apply the core Kubernetes manifests
      - name: Apply Core Kubernetes Manifests
        working-directory: ${{ env.K8S_DIR }}
        run: |
          kubectl apply -f serviceaccount.yaml
          kubectl apply -f complete-deploy.yaml

      # Deploy the ingress configuration
      - name: Deploy Ingress Configuration
        working-directory: ${{ env.K8S_DIR }}/frontendproxy
        run: |
          echo "Deploying ingress configuration..."
          kubectl apply -f ingress.yaml
          echo "Waiting for ingress to be ready..."
          kubectl wait --for=condition=ready ingress --all --timeout=300s || true

      # Verify the deployments
      - name: Verify Deployments
        run: |
          echo "Verifying Load Balancer Controller..."
          kubectl wait --for=condition=available --timeout=300s deployment -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system
          
          echo "Current cluster state:"
          kubectl get pods,services,ingress -A
