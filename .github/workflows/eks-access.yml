name: "EKS Access Management"

on:
  workflow_call:
    inputs:
      cluster_name:
        required: true
        type: string

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: eu-west-2
  TERRAFORM_DIR: ./terraform
  MAX_RETRIES: 3
  TF_VERSION: "1.5.0"

jobs:
  configure-access:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          # Remove any existing state
          rm -rf .terraform
          rm -f .terraform.lock.hcl
          
          # Initialize with backend config
          terraform init
          
          # Show current state
          echo "Current Terraform state:"
          terraform show || echo "No state found"

      - name: Download Terraform State
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          terraform state pull || echo "No remote state found"
          
          # List available outputs
          echo "Available Terraform outputs:"
          terraform output || echo "No outputs found"

      # Fallback to getting role ARNs from cluster
      - name: Get Role ARNs from Cluster
        run: |
          # Get node role from cluster
          NODE_ROLE=$(aws eks describe-nodegroup \
            --cluster-name ${{ inputs.cluster_name }} \
            --region ${{ env.AWS_REGION }} \
            --query 'nodegroups[0].nodeRole' \
            --output text)
          
          echo "NODE_ROLE=${NODE_ROLE}" >> $GITHUB_ENV
          
          # Get IAM roles based on naming convention
          ADMIN_ROLE=$(aws iam list-roles --query "Roles[?contains(RoleName, 'eks-admin')].Arn" --output text)
          DEV_ROLE=$(aws iam list-roles --query "Roles[?contains(RoleName, 'eks-developer')].Arn" --output text)
          READONLY_ROLE=$(aws iam list-roles --query "Roles[?contains(RoleName, 'eks-readonly')].Arn" --output text)
          
          echo "ADMIN_ROLE=${ADMIN_ROLE}" >> $GITHUB_ENV
          echo "DEV_ROLE=${DEV_ROLE}" >> $GITHUB_ENV
          echo "READONLY_ROLE=${READONLY_ROLE}" >> $GITHUB_ENV
          
          # Print roles for verification
          echo "Found roles:"
          echo "Node Role: ${NODE_ROLE}"
          echo "Admin Role: ${ADMIN_ROLE}"
          echo "Dev Role: ${DEV_ROLE}"
          echo "Readonly Role: ${READONLY_ROLE}"

      - name: Configure kubectl
        run: |
          aws eks update-kubeconfig \
            --name ${{ inputs.cluster_name }} \
            --region ${{ env.AWS_REGION }}
          
          # Verify connection
          kubectl get svc

      - name: Apply aws-auth ConfigMap
        run: |
          # Create ConfigMap with found roles
          cat aws-auth/configmap.yaml | \
          sed "s|\${NODE_ROLE}|${NODE_ROLE}|g" | \
          sed "s|\${ADMIN_ROLE}|${ADMIN_ROLE}|g" | \
          sed "s|\${DEV_ROLE}|${DEV_ROLE}|g" | \
          sed "s|\${READONLY_ROLE}|${READONLY_ROLE}|g" > aws-auth-new.yaml
          
          # Print ConfigMap for verification
          echo "Generated aws-auth ConfigMap:"
          cat aws-auth-new.yaml
          
          # Apply ConfigMap
          kubectl apply -f aws-auth-new.yaml